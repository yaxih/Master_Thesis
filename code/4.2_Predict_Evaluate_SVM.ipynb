{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e0a8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import distance\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from jellyfish._jellyfish import damerau_levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df68fe",
   "metadata": {},
   "source": [
    "# 1.Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27ddbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog = \"env_permit.csv\"\n",
    "eventlog_name = \"env_permit\"\n",
    "ori_eventlog=\"env_permit.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8063728-b83c-4d78-a9c0-1b54e0e62061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model = joblib.load(f'output_files/models/{eventlog_name}_SVM_best_model.pkl')\n",
    "#model = joblib.load(f'output_files/models/env_permit_lowV_SVM_best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e5f12-2bbf-4de8-838b-5611421c7334",
   "metadata": {},
   "source": [
    "# 1.1. Calculate Maxlen and Target labels\n",
    "\n",
    "to make sure the model is compile with different sized test set, the shape of the training set needs to be the same.\n",
    "Thus, maxlen is loaded as the maximum length of a trace from the original log, target_chars is loaded as all possible characters from the original log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba329646-b677-4248-92b7-7e6225ebbf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset env_permit.csv\n",
      "maxlen: 96\n",
      "total chars: 381, target chars: 382\n",
      "{0: '¢', 1: '£', 2: '¤', 3: '¥', 4: '¦', 5: '§', 6: '¨', 7: '©', 8: 'ª', 9: '«', 10: '¬', 11: '\\xad', 12: '®', 13: '¯', 14: '°', 15: '±', 16: '²', 17: '³', 18: '´', 19: 'µ', 20: '¶', 21: '·', 22: '¸', 23: '¹', 24: 'º', 25: '»', 26: '¼', 27: '½', 28: '¾', 29: '¿', 30: 'À', 31: 'Á', 32: 'Â', 33: 'Ã', 34: 'Ä', 35: 'Å', 36: 'Æ', 37: 'Ç', 38: 'È', 39: 'É', 40: 'Ê', 41: 'Ë', 42: 'Ì', 43: 'Í', 44: 'Î', 45: 'Ï', 46: 'Ð', 47: 'Ñ', 48: 'Ò', 49: 'Ó', 50: 'Ô', 51: 'Õ', 52: 'Ö', 53: '×', 54: 'Ø', 55: 'Ù', 56: 'Ú', 57: 'Û', 58: 'Ü', 59: 'Ý', 60: 'Þ', 61: 'ß', 62: 'à', 63: 'á', 64: 'â', 65: 'ã', 66: 'ä', 67: 'å', 68: 'æ', 69: 'ç', 70: 'è', 71: 'é', 72: 'ê', 73: 'ë', 74: 'ì', 75: 'í', 76: 'î', 77: 'ï', 78: 'ð', 79: 'ñ', 80: 'ò', 81: 'ó', 82: 'ô', 83: 'õ', 84: 'ö', 85: '÷', 86: 'ø', 87: 'ù', 88: 'ú', 89: 'û', 90: 'ü', 91: 'ý', 92: 'þ', 93: 'ÿ', 94: 'Ā', 95: 'ā', 96: 'Ă', 97: 'ă', 98: 'Ą', 99: 'ą', 100: 'Ć', 101: 'ć', 102: 'Ĉ', 103: 'ĉ', 104: 'Ċ', 105: 'ċ', 106: 'Č', 107: 'č', 108: 'Ď', 109: 'ď', 110: 'Đ', 111: 'đ', 112: 'Ē', 113: 'ē', 114: 'Ĕ', 115: 'ĕ', 116: 'Ė', 117: 'ė', 118: 'Ę', 119: 'ę', 120: 'Ě', 121: 'ě', 122: 'Ĝ', 123: 'ĝ', 124: 'Ğ', 125: 'ğ', 126: 'Ġ', 127: 'ġ', 128: 'Ģ', 129: 'ģ', 130: 'Ĥ', 131: 'ĥ', 132: 'Ħ', 133: 'ħ', 134: 'Ĩ', 135: 'ĩ', 136: 'Ī', 137: 'ī', 138: 'Ĭ', 139: 'ĭ', 140: 'Į', 141: 'į', 142: 'İ', 143: 'ı', 144: 'Ĳ', 145: 'ĳ', 146: 'Ĵ', 147: 'ĵ', 148: 'Ķ', 149: 'ķ', 150: 'ĸ', 151: 'Ĺ', 152: 'ĺ', 153: 'Ļ', 154: 'ļ', 155: 'Ľ', 156: 'ľ', 157: 'Ŀ', 158: 'ŀ', 159: 'Ł', 160: 'ł', 161: 'Ń', 162: 'ń', 163: 'Ņ', 164: 'ņ', 165: 'Ň', 166: 'ň', 167: 'ŉ', 168: 'Ŋ', 169: 'ŋ', 170: 'Ō', 171: 'ō', 172: 'Ŏ', 173: 'ŏ', 174: 'Ő', 175: 'ő', 176: 'Œ', 177: 'œ', 178: 'Ŕ', 179: 'ŕ', 180: 'Ŗ', 181: 'ŗ', 182: 'Ř', 183: 'ř', 184: 'Ś', 185: 'ś', 186: 'Ŝ', 187: 'ŝ', 188: 'Ş', 189: 'ş', 190: 'Š', 191: 'š', 192: 'Ţ', 193: 'ţ', 194: 'Ť', 195: 'ť', 196: 'Ŧ', 197: 'ŧ', 198: 'Ũ', 199: 'ũ', 200: 'Ū', 201: 'ū', 202: 'Ŭ', 203: 'ŭ', 204: 'Ů', 205: 'ů', 206: 'Ű', 207: 'ű', 208: 'Ų', 209: 'ų', 210: 'Ŵ', 211: 'ŵ', 212: 'Ŷ', 213: 'ŷ', 214: 'Ÿ', 215: 'Ź', 216: 'ź', 217: 'Ż', 218: 'ż', 219: 'Ž', 220: 'ž', 221: 'ſ', 222: 'ƀ', 223: 'Ɓ', 224: 'Ƃ', 225: 'ƃ', 226: 'Ƅ', 227: 'ƅ', 228: 'Ɔ', 229: 'Ƈ', 230: 'ƈ', 231: 'Ɖ', 232: 'Ɗ', 233: 'Ƌ', 234: 'ƌ', 235: 'ƍ', 236: 'Ǝ', 237: 'Ə', 238: 'Ɛ', 239: 'Ƒ', 240: 'ƒ', 241: 'Ɠ', 242: 'Ɣ', 243: 'ƕ', 244: 'Ɩ', 245: 'Ɨ', 246: 'Ƙ', 247: 'ƙ', 248: 'ƚ', 249: 'ƛ', 250: 'Ɯ', 251: 'Ɲ', 252: 'ƞ', 253: 'Ɵ', 254: 'Ơ', 255: 'ơ', 256: 'Ƣ', 257: 'ƣ', 258: 'Ƥ', 259: 'ƥ', 260: 'Ʀ', 261: 'Ƨ', 262: 'ƨ', 263: 'Ʃ', 264: 'ƪ', 265: 'ƫ', 266: 'Ƭ', 267: 'ƭ', 268: 'Ʈ', 269: 'Ư', 270: 'ư', 271: 'Ʊ', 272: 'Ʋ', 273: 'Ƴ', 274: 'ƴ', 275: 'Ƶ', 276: 'ƶ', 277: 'Ʒ', 278: 'Ƹ', 279: 'ƹ', 280: 'ƺ', 281: 'ƻ', 282: 'Ƽ', 283: 'ƽ', 284: 'ƾ', 285: 'ƿ', 286: 'ǀ', 287: 'ǁ', 288: 'ǂ', 289: 'ǃ', 290: 'Ǆ', 291: 'ǅ', 292: 'ǆ', 293: 'Ǉ', 294: 'ǈ', 295: 'ǉ', 296: 'Ǌ', 297: 'ǋ', 298: 'ǌ', 299: 'Ǎ', 300: 'ǎ', 301: 'Ǐ', 302: 'ǐ', 303: 'Ǒ', 304: 'ǒ', 305: 'Ǔ', 306: 'ǔ', 307: 'Ǖ', 308: 'ǖ', 309: 'Ǘ', 310: 'ǘ', 311: 'Ǚ', 312: 'ǚ', 313: 'Ǜ', 314: 'ǜ', 315: 'ǝ', 316: 'Ǟ', 317: 'ǟ', 318: 'Ǡ', 319: 'ǡ', 320: 'Ǣ', 321: 'ǣ', 322: 'Ǥ', 323: 'ǥ', 324: 'Ǧ', 325: 'ǧ', 326: 'Ǩ', 327: 'ǩ', 328: 'Ǫ', 329: 'ǫ', 330: 'Ǭ', 331: 'ǭ', 332: 'Ǯ', 333: 'ǯ', 334: 'ǰ', 335: 'Ǳ', 336: 'ǲ', 337: 'ǳ', 338: 'Ǵ', 339: 'ǵ', 340: 'Ƕ', 341: 'Ƿ', 342: 'Ǹ', 343: 'ǹ', 344: 'Ǻ', 345: 'ǻ', 346: 'Ǽ', 347: 'ǽ', 348: 'Ǿ', 349: 'ǿ', 350: 'Ȁ', 351: 'ȁ', 352: 'Ȃ', 353: 'ȃ', 354: 'Ȅ', 355: 'ȅ', 356: 'Ȇ', 357: 'ȇ', 358: 'Ȉ', 359: 'ȉ', 360: 'Ȋ', 361: 'ȋ', 362: 'Ȍ', 363: 'ȍ', 364: 'Ȏ', 365: 'ȏ', 366: 'Ȑ', 367: 'ȑ', 368: 'Ȓ', 369: 'ȓ', 370: 'Ȕ', 371: 'ȕ', 372: 'Ȗ', 373: 'ȗ', 374: 'Ș', 375: 'ș', 376: 'Ț', 377: 'ț', 378: 'Ȝ', 379: 'ȝ', 380: 'Ȟ'}\n"
     ]
    }
   ],
   "source": [
    "ori_csvfile = open('../data/%s' % ori_eventlog, 'r')\n",
    "print('original dataset',ori_eventlog)\n",
    "spamreader = csv.reader(ori_csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "lines = [] #these are all the activity seq\n",
    "timeseqs = [] #time sequences (differences between two events)\n",
    "timeseqs2 = [] #time sequences (differences between the current and first)\n",
    "\n",
    "#helper variables\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\n",
    "    t = datetime.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\n",
    "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = t - lasteventtime\n",
    "    timesincecasestart = t - casestarttime\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds  #time b/t current and last event\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds #time b/t current and starting event\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x+ '!',lines)) #add a delimiter symbol '!' to the end of each line\n",
    "maxlen = max(map(lambda x: len(x),lines)) #find maximum line size\n",
    "print('maxlen:',maxlen)\n",
    "# next lines here to get all possible characters for events and annotate them with numbers\n",
    "chars = list(map(lambda x: set(x),lines))\n",
    "chars = list(set().union(*chars))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "if '!' in chars:\n",
    "    chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "#get the target chars from the training set\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add7133",
   "metadata": {},
   "source": [
    "# 2.Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23f2d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divisor: 186908.12533381264\n",
      "divisor2: 5159390.008756163\n"
     ]
    }
   ],
   "source": [
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "caseids = []\n",
    "timeseqs = []\n",
    "timeseqs2 = []\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "for row in spamreader:\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        caseids.append(row[0])\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1\n",
    "\n",
    "#average time between events\n",
    "divisor = np.mean([item for sublist in timeseqs for item in sublist])\n",
    "print('divisor: {}'.format(divisor))\n",
    "#average time between current and starting events\n",
    "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist])\n",
    "print('divisor2: {}'.format(divisor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd8113fd-8fd1-4891-8e54-e42eda16f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "elems_per_fold = int(round(numlines/3))\n",
    "fold1 = lines[:elems_per_fold]\n",
    "fold1_c = caseids[:elems_per_fold]\n",
    "fold1_t = timeseqs[:elems_per_fold]\n",
    "fold1_t2 = timeseqs2[:elems_per_fold]\n",
    "\n",
    "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
    "fold2_c = caseids[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
    "\n",
    "lines = fold1 + fold2\n",
    "caseids = fold1_c + fold2_c\n",
    "lines_t = fold1_t + fold2_t\n",
    "lines_t2 = fold1_t2 + fold2_t2\n",
    "\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x+'!',lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6721f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "caseids = []\n",
    "timeseqs = []  # relative time since previous event\n",
    "timeseqs2 = [] # relative time since case start\n",
    "timeseqs3 = [] # absolute time of previous event\n",
    "times = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "for row in spamreader:\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        caseids.append(row[0])\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:        \n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "            timeseqs3.append(times3)\n",
    "        line = ''\n",
    "        times = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    #timediff = log(timediff+1)\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    times3.append(datetime.fromtimestamp(time.mktime(t)))\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "timeseqs3.append(times3)\n",
    "numlines+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dde429",
   "metadata": {},
   "source": [
    "### Using last 1/3 as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "baa23939",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold3 = lines[2*elems_per_fold:]\n",
    "fold3_c = caseids[2*elems_per_fold:]\n",
    "fold3_t = timeseqs[2*elems_per_fold:]\n",
    "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
    "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
    "#fold3_t4 = timeseqs4[2*elems_per_fold:]\n",
    "\n",
    "lines = fold3\n",
    "caseids = fold3_c\n",
    "lines_t = fold3_t\n",
    "lines_t2 = fold3_t2\n",
    "lines_t3 = fold3_t3\n",
    "#=lines_t4 = fold1_t4 + fold2_t4\n",
    "\n",
    "# set parameters\n",
    "predict_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697eda7",
   "metadata": {},
   "source": [
    "# 3.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03c43937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "# Define the function to encode the input sequence\n",
    "# 2 dimensional output for passing to the Random Forest model for prediction\n",
    "def encode(sentence, times, times3, maxlen=maxlen):\n",
    "    num_features = len(chars)+5\n",
    "    X = np.zeros((1, maxlen, num_features), dtype=np.float32)\n",
    "    leftpad = maxlen - len(sentence)\n",
    "    times2 = np.cumsum(times)\n",
    "    for t, char in enumerate(sentence):\n",
    "        midnight = times3[t].replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        timesincemidnight = times3[t] - midnight\n",
    "        multiset_abstraction = Counter(sentence[:t+1])\n",
    "        for c in chars:\n",
    "            if c == char:\n",
    "                X[0, t + leftpad, char_indices[c]] = 1\n",
    "        X[0, t + leftpad, len(chars)] = t + 1\n",
    "        X[0, t + leftpad, len(chars) + 1] = times[t] / divisor\n",
    "        X[0, t + leftpad, len(chars) + 2] = times2[t] / divisor2\n",
    "        X[0, t + leftpad, len(chars) + 3] = timesincemidnight.seconds / 86400\n",
    "        X[0, t + leftpad, len(chars) + 4] = times3[t].weekday() / 7\n",
    "\n",
    "    # Reshape X to be 2-dimensional\n",
    "    X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1bde354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get the predicted symbol from the output vector\n",
    "def getSymbol(predictions):\n",
    "    maxIndex = np.argmax(predictions)\n",
    "    symbol = target_indices_char[maxIndex]\n",
    "    return symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09d96e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_ahead_gt = []\n",
    "one_ahead_pred = []\n",
    "\n",
    "two_ahead_gt = []\n",
    "two_ahead_pred = []\n",
    "\n",
    "three_ahead_gt = []\n",
    "three_ahead_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5afa05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 37056 features, but SVC is expecting 29184 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m enc \u001b[38;5;241m=\u001b[39m encode(cropped_line, cropped_times, cropped_times3)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#print(enc)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m y_char \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#print('y_char:',y_char)\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\skmultilearn\\problem_transform\\lp.py:160\u001b[0m, in \u001b[0;36mLabelPowerset.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"Predict labels for X\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    binary indicator matrix with label assignments\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# this will be an np.array of integers representing classes\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m lp_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_input_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_transform(lp_prediction)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:791\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    789\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 791\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:414\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:592\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    589\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 592\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[0;32m    602\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 37056 features, but SVC is expecting 29184 features as input."
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "with open(f'output_files/results/SVM/{eventlog_name}_next_activity_%s' % eventlog, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow([\"CaseID\", \"Prefix length\", \"Groud truth\", \"Predicted\", \"Levenshtein\", \"Damerau\"])\n",
    "    for prefix_size in range(2,maxlen):\n",
    "        print(prefix_size)\n",
    "        for line, caseid, times, times3 in zip(lines, caseids, lines_t, lines_t3):\n",
    "            times.append(0)\n",
    "            cropped_line = ''.join(line[:prefix_size])\n",
    "            cropped_times = times[:prefix_size]\n",
    "            cropped_times3 = times3[:prefix_size]\n",
    "            if '!' in cropped_line:\n",
    "                continue # make no prediction for this case, since this case has ended already\n",
    "            ground_truth = ''.join(line[prefix_size:prefix_size+predict_size])\n",
    "            ground_truth_t = times[prefix_size:prefix_size+predict_size]\n",
    "            predicted = ''\n",
    "            for i in range(predict_size):\n",
    "                if len(ground_truth)<=i:\n",
    "                    continue\n",
    "                enc = encode(cropped_line, cropped_times, cropped_times3)\n",
    "                #print(enc)\n",
    "                y = model.predict(enc)\n",
    "                y_char = y.toarray()\n",
    "                #print('y_char:',y_char)\n",
    "                prediction = getSymbol(y_char)   \n",
    "                #print('prediction:' + prediction)\n",
    "                cropped_line += prediction\n",
    "                #print('cropped_line:' + cropped_line)\n",
    "                if prediction == '!': # end of case was just predicted, therefore, stop predicting further into the future\n",
    "                    print('! predicted, end case')\n",
    "                    break\n",
    "                predicted += prediction\n",
    "             \n",
    "            output = []\n",
    "            \n",
    "            if len(ground_truth)>0:\n",
    "                output.append(caseid)\n",
    "                output.append(prefix_size)\n",
    "                output.append(str(ground_truth).encode(\"utf-8\"))\n",
    "                output.append(str(predicted).encode(\"utf-8\"))\n",
    "                output.append(1 - distance.nlevenshtein(predicted, ground_truth))\n",
    "                dls = 1 - (damerau_levenshtein_distance(str(predicted), str(ground_truth)) / max(len(predicted),len(ground_truth)))\n",
    "                if dls<0:\n",
    "                    dls=0 # we encountered problems with Damerau-Levenshtein Similarity on some linux machines where the default character encoding of the operating system caused it to be negative, this should never be the case\n",
    "                output.append(dls)\n",
    "                output.append(1 - distance.jaccard(predicted, ground_truth))\n",
    "\n",
    "                spamwriter.writerow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9d271",
   "metadata": {},
   "source": [
    "# 4.Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a348db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "ground_truth = []\n",
    "predicted = []\n",
    "with open(f'output_files/results/SVM/{eventlog_name}_next_activity_%s' % eventlog) as file:\n",
    "    reader = csv.reader(file, delimiter=\",\")\n",
    "    # Skip the header\n",
    "    next(reader, None)\n",
    "    for row in reader:\n",
    "        # we do not evaluate the end activity of the case\n",
    "        if len(row) >= 4 and row[2] and row[3]:\n",
    "            ground_truth.append(row[2])\n",
    "            predicted.append(row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e2fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(ground_truth, predicted)\n",
    "print(\"Next activity accuracy: \", accuracy)\n",
    "print(classification_report(ground_truth, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert classification report to dataframe\n",
    "report = classification_report(ground_truth, predicted, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "df.to_csv(f'output_files/results/SVM/{eventlog_name}_Report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa20c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa554c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6279f85-016d-4e8f-9044-a85e5ded5f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e25eaa-69c8-46c2-a489-7515decb155a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbb282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
