{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0a8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import distance\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from jellyfish._jellyfish import damerau_levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df68fe",
   "metadata": {},
   "source": [
    "# 1.Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ddbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog = \"env_permit.csv\"\n",
    "eventlog_name = \"env_permit\"\n",
    "ori_eventlog=\"env_permit.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba4034-ecd6-498c-9b43-7069edce3f6c",
   "metadata": {},
   "source": [
    "# 1.1. Calculate Maxlen and Target labels\n",
    "\n",
    "to make sure the model is compile with different sized test set, the shape of the training set needs to be the same.\n",
    "Thus, maxlen is loaded as the maximum length of a trace from the original log, target_chars is loaded as all possible characters from the original log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c9f3fd-d6bf-4ab4-8911-ba31e5413b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset env_permit.csv\n",
      "maxlen: 96\n",
      "total chars: 381, target chars: 382\n",
      "{0: '¢', 1: '£', 2: '¤', 3: '¥', 4: '¦', 5: '§', 6: '¨', 7: '©', 8: 'ª', 9: '«', 10: '¬', 11: '\\xad', 12: '®', 13: '¯', 14: '°', 15: '±', 16: '²', 17: '³', 18: '´', 19: 'µ', 20: '¶', 21: '·', 22: '¸', 23: '¹', 24: 'º', 25: '»', 26: '¼', 27: '½', 28: '¾', 29: '¿', 30: 'À', 31: 'Á', 32: 'Â', 33: 'Ã', 34: 'Ä', 35: 'Å', 36: 'Æ', 37: 'Ç', 38: 'È', 39: 'É', 40: 'Ê', 41: 'Ë', 42: 'Ì', 43: 'Í', 44: 'Î', 45: 'Ï', 46: 'Ð', 47: 'Ñ', 48: 'Ò', 49: 'Ó', 50: 'Ô', 51: 'Õ', 52: 'Ö', 53: '×', 54: 'Ø', 55: 'Ù', 56: 'Ú', 57: 'Û', 58: 'Ü', 59: 'Ý', 60: 'Þ', 61: 'ß', 62: 'à', 63: 'á', 64: 'â', 65: 'ã', 66: 'ä', 67: 'å', 68: 'æ', 69: 'ç', 70: 'è', 71: 'é', 72: 'ê', 73: 'ë', 74: 'ì', 75: 'í', 76: 'î', 77: 'ï', 78: 'ð', 79: 'ñ', 80: 'ò', 81: 'ó', 82: 'ô', 83: 'õ', 84: 'ö', 85: '÷', 86: 'ø', 87: 'ù', 88: 'ú', 89: 'û', 90: 'ü', 91: 'ý', 92: 'þ', 93: 'ÿ', 94: 'Ā', 95: 'ā', 96: 'Ă', 97: 'ă', 98: 'Ą', 99: 'ą', 100: 'Ć', 101: 'ć', 102: 'Ĉ', 103: 'ĉ', 104: 'Ċ', 105: 'ċ', 106: 'Č', 107: 'č', 108: 'Ď', 109: 'ď', 110: 'Đ', 111: 'đ', 112: 'Ē', 113: 'ē', 114: 'Ĕ', 115: 'ĕ', 116: 'Ė', 117: 'ė', 118: 'Ę', 119: 'ę', 120: 'Ě', 121: 'ě', 122: 'Ĝ', 123: 'ĝ', 124: 'Ğ', 125: 'ğ', 126: 'Ġ', 127: 'ġ', 128: 'Ģ', 129: 'ģ', 130: 'Ĥ', 131: 'ĥ', 132: 'Ħ', 133: 'ħ', 134: 'Ĩ', 135: 'ĩ', 136: 'Ī', 137: 'ī', 138: 'Ĭ', 139: 'ĭ', 140: 'Į', 141: 'į', 142: 'İ', 143: 'ı', 144: 'Ĳ', 145: 'ĳ', 146: 'Ĵ', 147: 'ĵ', 148: 'Ķ', 149: 'ķ', 150: 'ĸ', 151: 'Ĺ', 152: 'ĺ', 153: 'Ļ', 154: 'ļ', 155: 'Ľ', 156: 'ľ', 157: 'Ŀ', 158: 'ŀ', 159: 'Ł', 160: 'ł', 161: 'Ń', 162: 'ń', 163: 'Ņ', 164: 'ņ', 165: 'Ň', 166: 'ň', 167: 'ŉ', 168: 'Ŋ', 169: 'ŋ', 170: 'Ō', 171: 'ō', 172: 'Ŏ', 173: 'ŏ', 174: 'Ő', 175: 'ő', 176: 'Œ', 177: 'œ', 178: 'Ŕ', 179: 'ŕ', 180: 'Ŗ', 181: 'ŗ', 182: 'Ř', 183: 'ř', 184: 'Ś', 185: 'ś', 186: 'Ŝ', 187: 'ŝ', 188: 'Ş', 189: 'ş', 190: 'Š', 191: 'š', 192: 'Ţ', 193: 'ţ', 194: 'Ť', 195: 'ť', 196: 'Ŧ', 197: 'ŧ', 198: 'Ũ', 199: 'ũ', 200: 'Ū', 201: 'ū', 202: 'Ŭ', 203: 'ŭ', 204: 'Ů', 205: 'ů', 206: 'Ű', 207: 'ű', 208: 'Ų', 209: 'ų', 210: 'Ŵ', 211: 'ŵ', 212: 'Ŷ', 213: 'ŷ', 214: 'Ÿ', 215: 'Ź', 216: 'ź', 217: 'Ż', 218: 'ż', 219: 'Ž', 220: 'ž', 221: 'ſ', 222: 'ƀ', 223: 'Ɓ', 224: 'Ƃ', 225: 'ƃ', 226: 'Ƅ', 227: 'ƅ', 228: 'Ɔ', 229: 'Ƈ', 230: 'ƈ', 231: 'Ɖ', 232: 'Ɗ', 233: 'Ƌ', 234: 'ƌ', 235: 'ƍ', 236: 'Ǝ', 237: 'Ə', 238: 'Ɛ', 239: 'Ƒ', 240: 'ƒ', 241: 'Ɠ', 242: 'Ɣ', 243: 'ƕ', 244: 'Ɩ', 245: 'Ɨ', 246: 'Ƙ', 247: 'ƙ', 248: 'ƚ', 249: 'ƛ', 250: 'Ɯ', 251: 'Ɲ', 252: 'ƞ', 253: 'Ɵ', 254: 'Ơ', 255: 'ơ', 256: 'Ƣ', 257: 'ƣ', 258: 'Ƥ', 259: 'ƥ', 260: 'Ʀ', 261: 'Ƨ', 262: 'ƨ', 263: 'Ʃ', 264: 'ƪ', 265: 'ƫ', 266: 'Ƭ', 267: 'ƭ', 268: 'Ʈ', 269: 'Ư', 270: 'ư', 271: 'Ʊ', 272: 'Ʋ', 273: 'Ƴ', 274: 'ƴ', 275: 'Ƶ', 276: 'ƶ', 277: 'Ʒ', 278: 'Ƹ', 279: 'ƹ', 280: 'ƺ', 281: 'ƻ', 282: 'Ƽ', 283: 'ƽ', 284: 'ƾ', 285: 'ƿ', 286: 'ǀ', 287: 'ǁ', 288: 'ǂ', 289: 'ǃ', 290: 'Ǆ', 291: 'ǅ', 292: 'ǆ', 293: 'Ǉ', 294: 'ǈ', 295: 'ǉ', 296: 'Ǌ', 297: 'ǋ', 298: 'ǌ', 299: 'Ǎ', 300: 'ǎ', 301: 'Ǐ', 302: 'ǐ', 303: 'Ǒ', 304: 'ǒ', 305: 'Ǔ', 306: 'ǔ', 307: 'Ǖ', 308: 'ǖ', 309: 'Ǘ', 310: 'ǘ', 311: 'Ǚ', 312: 'ǚ', 313: 'Ǜ', 314: 'ǜ', 315: 'ǝ', 316: 'Ǟ', 317: 'ǟ', 318: 'Ǡ', 319: 'ǡ', 320: 'Ǣ', 321: 'ǣ', 322: 'Ǥ', 323: 'ǥ', 324: 'Ǧ', 325: 'ǧ', 326: 'Ǩ', 327: 'ǩ', 328: 'Ǫ', 329: 'ǫ', 330: 'Ǭ', 331: 'ǭ', 332: 'Ǯ', 333: 'ǯ', 334: 'ǰ', 335: 'Ǳ', 336: 'ǲ', 337: 'ǳ', 338: 'Ǵ', 339: 'ǵ', 340: 'Ƕ', 341: 'Ƿ', 342: 'Ǹ', 343: 'ǹ', 344: 'Ǻ', 345: 'ǻ', 346: 'Ǽ', 347: 'ǽ', 348: 'Ǿ', 349: 'ǿ', 350: 'Ȁ', 351: 'ȁ', 352: 'Ȃ', 353: 'ȃ', 354: 'Ȅ', 355: 'ȅ', 356: 'Ȇ', 357: 'ȇ', 358: 'Ȉ', 359: 'ȉ', 360: 'Ȋ', 361: 'ȋ', 362: 'Ȍ', 363: 'ȍ', 364: 'Ȏ', 365: 'ȏ', 366: 'Ȑ', 367: 'ȑ', 368: 'Ȓ', 369: 'ȓ', 370: 'Ȕ', 371: 'ȕ', 372: 'Ȗ', 373: 'ȗ', 374: 'Ș', 375: 'ș', 376: 'Ț', 377: 'ț', 378: 'Ȝ', 379: 'ȝ', 380: 'Ȟ'}\n"
     ]
    }
   ],
   "source": [
    "ori_csvfile = open('../data/%s' % ori_eventlog, 'r')\n",
    "print('original dataset',ori_eventlog)\n",
    "spamreader = csv.reader(ori_csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "lines = [] #these are all the activity seq\n",
    "timeseqs = [] #time sequences (differences between two events)\n",
    "timeseqs2 = [] #time sequences (differences between the current and first)\n",
    "\n",
    "#helper variables\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\n",
    "    t = datetime.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\n",
    "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = t - lasteventtime\n",
    "    timesincecasestart = t - casestarttime\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds  #time b/t current and last event\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds #time b/t current and starting event\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x+ '!',lines)) #add a delimiter symbol '!' to the end of each line\n",
    "maxlen = max(map(lambda x: len(x),lines)) #find maximum line size\n",
    "print('maxlen:',maxlen)\n",
    "# next lines here to get all possible characters for events and annotate them with numbers\n",
    "chars = list(map(lambda x: set(x),lines))\n",
    "chars = list(set().union(*chars))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "if '!' in chars:\n",
    "    chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "#get the target chars from the training set\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab23378",
   "metadata": {},
   "source": [
    "# 2.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027e6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "lines = [] #these are all the activity seq\n",
    "timeseqs = [] #time sequences (differences between two events)\n",
    "timeseqs2 = [] #time sequences (differences between the current and first)\n",
    "\n",
    "#helper variables\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e1e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\n",
    "    t = datetime.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\n",
    "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = t - lasteventtime\n",
    "    timesincecasestart = t - casestarttime\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds  #time b/t current and last event\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds #time b/t current and starting event\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91178c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing processed data\n",
    "#print(lines)\n",
    "#print(timeseqs)\n",
    "#print(timeseqs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7048d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divisor: 186908.12533381264\n",
      "divisor2: 5159390.008756163\n"
     ]
    }
   ],
   "source": [
    "#average time between events\n",
    "divisor = np.mean([item for sublist in timeseqs for item in sublist]) \n",
    "print('divisor: {}'.format(divisor))\n",
    "#average time between current and starting events\n",
    "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist]) \n",
    "print('divisor2: {}'.format(divisor2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cb884",
   "metadata": {},
   "source": [
    "# 3. Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bc4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "timeseqs = []\n",
    "timeseqs2 = []\n",
    "timeseqs3 = []\n",
    "timeseqs4 = []\n",
    "times = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "times4 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5147c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in spamreader:\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "            timeseqs3.append(times3)\n",
    "            timeseqs4.append(times4)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        times3 = []\n",
    "        times4 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    timediff3 = timesincemidnight.seconds #this leaves only time even occurred after midnight\n",
    "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    times3.append(timediff3)\n",
    "    times4.append(timediff4)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "timeseqs3.append(times3)\n",
    "timeseqs4.append(times4)\n",
    "numlines+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7714ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold 1\n",
    "elems_per_fold = int(round(numlines/3)) #calculate the number of elements per fold\n",
    "fold1 = lines[:elems_per_fold]\n",
    "fold1_t = timeseqs[:elems_per_fold]\n",
    "fold1_t2 = timeseqs2[:elems_per_fold]\n",
    "fold1_t3 = timeseqs3[:elems_per_fold]\n",
    "fold1_t4 = timeseqs4[:elems_per_fold]\n",
    "  \n",
    "# fold 2\n",
    "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t3 = timeseqs3[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t4 = timeseqs4[elems_per_fold:2*elems_per_fold]\n",
    "        \n",
    "# fold 3\n",
    "fold3 = lines[2*elems_per_fold:]\n",
    "fold3_t = timeseqs[2*elems_per_fold:]\n",
    "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
    "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
    "fold3_t4 = timeseqs4[2*elems_per_fold:]\n",
    "\n",
    "lines = fold1 + fold2\n",
    "lines_t = fold1_t + fold2_t\n",
    "lines_t2 = fold1_t2 + fold2_t2\n",
    "lines_t3 = fold1_t3 + fold2_t3\n",
    "lines_t4 = fold1_t4 + fold2_t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52fe7a3a-ce72-447d-b9a4-9666f0315cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save training set\n",
    "df_train = pd.DataFrame({\n",
    "    'lines': lines,\n",
    "    'times': lines_t,\n",
    "    'times2': lines_t2,\n",
    "    'times3': lines_t3,\n",
    "    'times4': lines_t4\n",
    "    })\n",
    "# Convert times columns to strings\n",
    "df_train['times'] = df_train['times'].apply(lambda x: ','.join(map(str, x)))\n",
    "df_train['times2'] = df_train['times2'].apply(lambda x: ','.join(map(str, x)))\n",
    "df_train['times3'] = df_train['times3'].apply(lambda x: ','.join(map(str, x)))\n",
    "df_train['times4'] = df_train['times4'].apply(lambda x: ','.join(map(str, x)))\n",
    "# Decode the 'lines' column from ASCII to integers\n",
    "df_train['lines'] = df_train['lines'].apply(lambda x: ''.join(str(ord(c) - ascii_offset) for c in x))\n",
    "df_train.to_excel(f'output_files/folds/{eventlog_name}_train_set.xlsx', engine='xlsxwriter', index=False)\n",
    "\n",
    "#save test set\n",
    "df_test = pd.DataFrame({\n",
    "    'lines': fold3,\n",
    "    'times': fold3_t,\n",
    "    'times2': fold3_t2,\n",
    "    'times3': fold3_t3,\n",
    "    'times4': fold3_t4\n",
    "    })\n",
    "# Convert times columns to strings\n",
    "df_test['times'] = df_test['times'].apply(lambda x: ','.join(map(str, x)))\n",
    "df_test['times2'] = df_test['times2'].apply(lambda x: ','.join(map(str, x)))\n",
    "df_test['times3'] = df_test['times3'].apply(lambda x: ','.join(map(str, x)))\n",
    "df_test['times4'] = df_test['times4'].apply(lambda x: ','.join(map(str, x)))\n",
    "# Decode the 'lines' column from ASCII to integers\n",
    "df_test['lines'] = df_test['lines'].apply(lambda x: ''.join(str(ord(c) - ascii_offset) for c in x))\n",
    "df_test.to_excel(f'output_files/folds/{eventlog_name}_test_set.xlsx', engine='xlsxwriter', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e359f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 24600\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x+'!', lines))\n",
    "\n",
    "sentences_t = []\n",
    "sentences_t2 = []\n",
    "sentences_t3 = []\n",
    "sentences_t4 = []\n",
    "next_chars_t = []\n",
    "next_chars_t2 = []\n",
    "next_chars_t3 = []\n",
    "next_chars_t4 = []\n",
    "\n",
    "for line, line_t, line_t2, line_t3, line_t4 in zip(lines, lines_t, lines_t2, lines_t3, lines_t4):\n",
    "    for i in range(0, len(line), step):\n",
    "        if i==0:\n",
    "            continue\n",
    "\n",
    "        #we add iteratively, first symbol of the line, then two first, three...\n",
    "        sentences.append(line[0: i])\n",
    "        sentences_t.append(line_t[0:i])\n",
    "        sentences_t2.append(line_t2[0:i])\n",
    "        sentences_t3.append(line_t3[0:i])\n",
    "        sentences_t4.append(line_t4[0:i])\n",
    "        next_chars.append(line[i])\n",
    "\n",
    "        if i==len(line)-1: # special case to deal time of end character\n",
    "            next_chars_t.append(0)\n",
    "            next_chars_t2.append(0)\n",
    "            next_chars_t3.append(0)\n",
    "            next_chars_t4.append(0)\n",
    "        else:\n",
    "            next_chars_t.append(line_t[i])\n",
    "            next_chars_t2.append(line_t2[i])\n",
    "            next_chars_t3.append(line_t3[i])\n",
    "            next_chars_t4.append(line_t4[i])\n",
    "\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239d0a3",
   "metadata": {},
   "source": [
    "# 4. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "928f529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "num features: 386\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "num_features = len(chars)+5\n",
    "print('num features: {}'.format(num_features))\n",
    "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\n",
    "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\n",
    "y_t = np.zeros((len(sentences)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    leftpad = maxlen-len(sentence)\n",
    "    next_t = next_chars_t[i]\n",
    "    sentence_t = sentences_t[i]\n",
    "    sentence_t2 = sentences_t2[i]\n",
    "    sentence_t3 = sentences_t3[i]\n",
    "    sentence_t4 = sentences_t4[i]\n",
    "    for t, char in enumerate(sentence):\n",
    "        multiset_abstraction = Counter(sentence[:t+1])\n",
    "        for c in chars:\n",
    "            if c==char: #this will encode present events to the right places\n",
    "                X[i, t+leftpad, char_indices[c]] = 1\n",
    "        X[i, t+leftpad, len(chars)] = t+1\n",
    "        X[i, t+leftpad, len(chars)+1] = sentence_t[t]/divisor\n",
    "        X[i, t+leftpad, len(chars)+2] = sentence_t2[t]/divisor2\n",
    "        X[i, t+leftpad, len(chars)+3] = sentence_t3[t]/86400\n",
    "        X[i, t+leftpad, len(chars)+4] = sentence_t4[t]/7\n",
    "    for c in target_chars:\n",
    "        if c==next_chars[i]:\n",
    "            y_a[i, target_char_indices[c]] = 1-softness\n",
    "        else:\n",
    "            y_a[i, target_char_indices[c]] = softness/(len(target_chars)-1)\n",
    "    y_t[i] = next_t/divisor\n",
    "    np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa6699",
   "metadata": {},
   "source": [
    "# 5. Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19430748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization,Flatten\n",
    "from tensorflow.keras.optimizers import Nadam, legacy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1815163d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 96, 386) dtype=float32 (created by layer 'main_input')>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "main_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21074ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = Flatten()(main_input)\n",
    "dense1 = Dense(100, activation='relu', kernel_initializer='glorot_uniform')(flatten_layer)\n",
    "dense1 = BatchNormalization()(dense1)\n",
    "dense2 = Dense(100, activation='relu', kernel_initializer='glorot_uniform')(dense1)\n",
    "dense2 = BatchNormalization()(dense2)\n",
    "act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(dense2)\n",
    "#time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2854386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "model = Model(inputs=[main_input], outputs=[act_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "202e5fb4-18db-4150-a3a8-a5a01130edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = os.path.join('output_files/models', f'{eventlog_name}_DNN_model_{{epoch:02d}}-{{val_loss:.2f}}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b14183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\I541451\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\legacy\\nadam.py:89: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt = legacy.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.004, clipvalue=3)\n",
    "#model.compile(loss={'act_output': 'categorical_crossentropy', 'time_output': 'mae'}, optimizer=opt)\n",
    "model.compile(loss={'act_output': 'categorical_crossentropy'}, optimizer=opt)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f30c5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "205/205 - 10s - loss: 3.0768 - val_loss: 4.5316 - lr: 0.0020 - 10s/epoch - 49ms/step\n",
      "Epoch 2/500\n",
      "205/205 - 8s - loss: 1.9481 - val_loss: 4.0104 - lr: 0.0020 - 8s/epoch - 40ms/step\n",
      "Epoch 3/500\n",
      "205/205 - 8s - loss: 1.5747 - val_loss: 3.9915 - lr: 0.0020 - 8s/epoch - 37ms/step\n",
      "Epoch 4/500\n",
      "205/205 - 8s - loss: 1.3210 - val_loss: 3.8148 - lr: 0.0020 - 8s/epoch - 37ms/step\n",
      "Epoch 5/500\n",
      "205/205 - 8s - loss: 1.1533 - val_loss: 4.0430 - lr: 0.0020 - 8s/epoch - 37ms/step\n",
      "Epoch 6/500\n",
      "205/205 - 22s - loss: 1.0298 - val_loss: 3.9926 - lr: 0.0020 - 22s/epoch - 106ms/step\n",
      "Epoch 7/500\n",
      "205/205 - 7s - loss: 0.9173 - val_loss: 4.6048 - lr: 0.0020 - 7s/epoch - 36ms/step\n",
      "Epoch 8/500\n",
      "205/205 - 8s - loss: 0.8224 - val_loss: 4.2708 - lr: 0.0020 - 8s/epoch - 40ms/step\n",
      "Epoch 9/500\n",
      "205/205 - 10s - loss: 0.7606 - val_loss: 4.4768 - lr: 0.0020 - 10s/epoch - 48ms/step\n",
      "Epoch 10/500\n",
      "205/205 - 9s - loss: 0.6783 - val_loss: 4.5940 - lr: 0.0020 - 9s/epoch - 43ms/step\n",
      "Epoch 11/500\n",
      "205/205 - 9s - loss: 0.6427 - val_loss: 4.9383 - lr: 0.0020 - 9s/epoch - 43ms/step\n",
      "Epoch 12/500\n",
      "205/205 - 9s - loss: 0.5955 - val_loss: 4.7809 - lr: 0.0020 - 9s/epoch - 42ms/step\n",
      "Epoch 13/500\n",
      "205/205 - 9s - loss: 0.5610 - val_loss: 4.9032 - lr: 0.0020 - 9s/epoch - 43ms/step\n",
      "Epoch 14/500\n",
      "205/205 - 9s - loss: 0.5402 - val_loss: 5.5491 - lr: 0.0020 - 9s/epoch - 43ms/step\n",
      "Epoch 15/500\n",
      "205/205 - 9s - loss: 0.4128 - val_loss: 4.8118 - lr: 0.0010 - 9s/epoch - 43ms/step\n",
      "Epoch 16/500\n",
      "205/205 - 9s - loss: 0.3633 - val_loss: 5.1157 - lr: 0.0010 - 9s/epoch - 44ms/step\n",
      "Epoch 17/500\n",
      "205/205 - 9s - loss: 0.3447 - val_loss: 4.9717 - lr: 0.0010 - 9s/epoch - 43ms/step\n",
      "Epoch 18/500\n",
      "205/205 - 9s - loss: 0.3346 - val_loss: 5.2400 - lr: 0.0010 - 9s/epoch - 46ms/step\n",
      "Epoch 19/500\n",
      "205/205 - 9s - loss: 0.3304 - val_loss: 5.2538 - lr: 0.0010 - 9s/epoch - 43ms/step\n",
      "Epoch 20/500\n",
      "205/205 - 9s - loss: 0.3220 - val_loss: 5.1658 - lr: 0.0010 - 9s/epoch - 45ms/step\n",
      "Epoch 21/500\n",
      "205/205 - 9s - loss: 0.3258 - val_loss: 5.4312 - lr: 0.0010 - 9s/epoch - 44ms/step\n",
      "Epoch 22/500\n",
      "205/205 - 10s - loss: 0.3095 - val_loss: 5.3174 - lr: 0.0010 - 10s/epoch - 47ms/step\n",
      "Epoch 23/500\n",
      "205/205 - 10s - loss: 0.3008 - val_loss: 5.9166 - lr: 0.0010 - 10s/epoch - 47ms/step\n",
      "Epoch 24/500\n",
      "205/205 - 10s - loss: 0.3003 - val_loss: 5.8108 - lr: 0.0010 - 10s/epoch - 49ms/step\n",
      "Epoch 25/500\n",
      "205/205 - 10s - loss: 0.2560 - val_loss: 5.5966 - lr: 5.0000e-04 - 10s/epoch - 47ms/step\n",
      "Epoch 26/500\n",
      "205/205 - 10s - loss: 0.2397 - val_loss: 5.5029 - lr: 5.0000e-04 - 10s/epoch - 48ms/step\n",
      "Epoch 27/500\n",
      "205/205 - 10s - loss: 0.2355 - val_loss: 5.7353 - lr: 5.0000e-04 - 10s/epoch - 48ms/step\n",
      "Epoch 28/500\n",
      "205/205 - 10s - loss: 0.2313 - val_loss: 5.7212 - lr: 5.0000e-04 - 10s/epoch - 51ms/step\n",
      "Epoch 29/500\n",
      "205/205 - 11s - loss: 0.2313 - val_loss: 5.6667 - lr: 5.0000e-04 - 11s/epoch - 54ms/step\n",
      "Epoch 30/500\n",
      "205/205 - 12s - loss: 0.2282 - val_loss: 5.8930 - lr: 5.0000e-04 - 12s/epoch - 60ms/step\n",
      "Epoch 31/500\n",
      "205/205 - 11s - loss: 0.2287 - val_loss: 5.7821 - lr: 5.0000e-04 - 11s/epoch - 55ms/step\n",
      "Epoch 32/500\n",
      "205/205 - 11s - loss: 0.2245 - val_loss: 5.8190 - lr: 5.0000e-04 - 11s/epoch - 54ms/step\n",
      "Epoch 33/500\n",
      "205/205 - 11s - loss: 0.2255 - val_loss: 5.9430 - lr: 5.0000e-04 - 11s/epoch - 53ms/step\n",
      "Epoch 34/500\n",
      "205/205 - 11s - loss: 0.2172 - val_loss: 5.9225 - lr: 5.0000e-04 - 11s/epoch - 54ms/step\n",
      "Epoch 35/500\n",
      "205/205 - 11s - loss: 0.2022 - val_loss: 5.8818 - lr: 2.5000e-04 - 11s/epoch - 54ms/step\n",
      "Epoch 36/500\n",
      "205/205 - 9s - loss: 0.1984 - val_loss: 5.9778 - lr: 2.5000e-04 - 9s/epoch - 44ms/step\n",
      "Epoch 37/500\n",
      "205/205 - 7s - loss: 0.1928 - val_loss: 5.8275 - lr: 2.5000e-04 - 7s/epoch - 34ms/step\n",
      "Epoch 38/500\n",
      "205/205 - 7s - loss: 0.1930 - val_loss: 6.1218 - lr: 2.5000e-04 - 7s/epoch - 34ms/step\n",
      "Epoch 39/500\n",
      "205/205 - 7s - loss: 0.1941 - val_loss: 5.9823 - lr: 2.5000e-04 - 7s/epoch - 34ms/step\n",
      "Epoch 40/500\n",
      "205/205 - 7s - loss: 0.1906 - val_loss: 6.0572 - lr: 2.5000e-04 - 7s/epoch - 35ms/step\n",
      "Epoch 41/500\n",
      "205/205 - 7s - loss: 0.1909 - val_loss: 6.0665 - lr: 2.5000e-04 - 7s/epoch - 35ms/step\n",
      "Epoch 42/500\n",
      "205/205 - 7s - loss: 0.1874 - val_loss: 6.1939 - lr: 2.5000e-04 - 7s/epoch - 35ms/step\n",
      "Epoch 43/500\n",
      "205/205 - 7s - loss: 0.1875 - val_loss: 6.2528 - lr: 2.5000e-04 - 7s/epoch - 35ms/step\n",
      "Epoch 44/500\n",
      "205/205 - 7s - loss: 0.1868 - val_loss: 6.0108 - lr: 2.5000e-04 - 7s/epoch - 35ms/step\n",
      "Epoch 45/500\n",
      "205/205 - 8s - loss: 0.1806 - val_loss: 6.1503 - lr: 1.2500e-04 - 8s/epoch - 38ms/step\n",
      "Epoch 46/500\n",
      "205/205 - 7s - loss: 0.1774 - val_loss: 6.1715 - lr: 1.2500e-04 - 7s/epoch - 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29183962280>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)\n",
    "model.fit(X, {'act_output':y_a}, validation_split=0.2, verbose=2, callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171adb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdf26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9670641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
