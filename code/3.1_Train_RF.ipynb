{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0a8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import distance\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from jellyfish._jellyfish import damerau_levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df68fe",
   "metadata": {},
   "source": [
    "# 1.Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ddbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog = \"Preprocessed_bpi_12_w_lowV.csv\"\n",
    "eventlog_name = \"bpi_12_w_lowV\"\n",
    "ori_eventlog=\"bpi_12_w.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32b2bd-8029-4e60-a2e2-7f60f5b65b5d",
   "metadata": {},
   "source": [
    "# 1.1. Calculate Maxlen and Target labels\n",
    "\n",
    "to make sure the model is compile with different sized test set, the shape of the training set needs to be the same.\n",
    "Thus, maxlen is loaded as the maximum length of a trace from the original log, target_chars is loaded as all possible characters from the original log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493a1706-24db-4110-a19d-64c2436248f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset bpi_12_w.csv\n",
      "maxlen: 75\n",
      "total chars: 6, target chars: 7\n",
      "{0: '¢', 1: '£', 2: '¤', 3: '¥', 4: '¦', 5: '§'}\n"
     ]
    }
   ],
   "source": [
    "ori_csvfile = open('../data/%s' % ori_eventlog, 'r')\n",
    "print('original dataset',ori_eventlog)\n",
    "spamreader = csv.reader(ori_csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "lines = [] #these are all the activity seq\n",
    "timeseqs = [] #time sequences (differences between two events)\n",
    "timeseqs2 = [] #time sequences (differences between the current and first)\n",
    "\n",
    "#helper variables\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\n",
    "    t = datetime.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\n",
    "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = t - lasteventtime\n",
    "    timesincecasestart = t - casestarttime\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds  #time b/t current and last event\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds #time b/t current and starting event\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x+ '!',lines)) #add a delimiter symbol '!' to the end of each line\n",
    "maxlen = max(map(lambda x: len(x),lines)) #find maximum line size\n",
    "print('maxlen:',maxlen)\n",
    "# next lines here to get all possible characters for events and annotate them with numbers\n",
    "chars = list(map(lambda x: set(x),lines))\n",
    "chars = list(set().union(*chars))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "if '!' in chars:\n",
    "    chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "#get the target chars from the training set\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab23378",
   "metadata": {},
   "source": [
    "# 2.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027e6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "lines = [] #these are all the activity seq\n",
    "timeseqs = [] #time sequences (differences between two events)\n",
    "timeseqs2 = [] #time sequences (differences between the current and first)\n",
    "\n",
    "#helper variables\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e1e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\n",
    "    t = datetime.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\n",
    "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = t - lasteventtime\n",
    "    timesincecasestart = t - casestarttime\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds  #time b/t current and last event\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds #time b/t current and starting event\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91178c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing processed data\n",
    "#print(lines)\n",
    "#print(timeseqs)\n",
    "#print(timeseqs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7048d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divisor: 141536.56641168954\n",
      "divisor2: 738231.8796520612\n"
     ]
    }
   ],
   "source": [
    "#average time between events\n",
    "divisor = np.mean([item for sublist in timeseqs for item in sublist]) \n",
    "print('divisor: {}'.format(divisor))\n",
    "#average time between current and starting events\n",
    "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist]) \n",
    "print('divisor2: {}'.format(divisor2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cb884",
   "metadata": {},
   "source": [
    "# 3. Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bc4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "timeseqs = []\n",
    "timeseqs2 = []\n",
    "timeseqs3 = []\n",
    "timeseqs4 = []\n",
    "times = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "times4 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5147c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in spamreader:\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "            timeseqs3.append(times3)\n",
    "            timeseqs4.append(times4)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        times3 = []\n",
    "        times4 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    timediff3 = timesincemidnight.seconds #this leaves only time even occurred after midnight\n",
    "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    times3.append(timediff3)\n",
    "    times4.append(timediff4)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "timeseqs3.append(times3)\n",
    "timeseqs4.append(times4)\n",
    "numlines+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7714ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold 1\n",
    "elems_per_fold = int(round(numlines/3)) #calculate the number of elements per fold\n",
    "fold1 = lines[:elems_per_fold]\n",
    "fold1_t = timeseqs[:elems_per_fold]\n",
    "fold1_t2 = timeseqs2[:elems_per_fold]\n",
    "fold1_t3 = timeseqs3[:elems_per_fold]\n",
    "fold1_t4 = timeseqs4[:elems_per_fold]\n",
    "        \n",
    "# fold 2\n",
    "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t3 = timeseqs3[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t4 = timeseqs4[elems_per_fold:2*elems_per_fold]\n",
    "\n",
    "        \n",
    "# fold 3\n",
    "fold3 = lines[2*elems_per_fold:]\n",
    "fold3_t = timeseqs[2*elems_per_fold:]\n",
    "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
    "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
    "fold3_t4 = timeseqs4[2*elems_per_fold:]\n",
    "\n",
    "\n",
    "lines = fold1 + fold2\n",
    "lines_t = fold1_t + fold2_t\n",
    "lines_t2 = fold1_t2 + fold2_t2\n",
    "lines_t3 = fold1_t3 + fold2_t3\n",
    "lines_t4 = fold1_t4 + fold2_t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e359f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 19516\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x+'!', lines))\n",
    "\n",
    "sentences_t = []\n",
    "sentences_t2 = []\n",
    "sentences_t3 = []\n",
    "sentences_t4 = []\n",
    "next_chars_t = []\n",
    "next_chars_t2 = []\n",
    "next_chars_t3 = []\n",
    "next_chars_t4 = []\n",
    "\n",
    "for line, line_t, line_t2, line_t3, line_t4 in zip(lines, lines_t, lines_t2, lines_t3, lines_t4):\n",
    "    for i in range(0, len(line), step):\n",
    "        if i==0:\n",
    "            continue\n",
    "\n",
    "        #we add iteratively, first symbol of the line, then two first, three...\n",
    "        sentences.append(line[0: i])\n",
    "        sentences_t.append(line_t[0:i])\n",
    "        sentences_t2.append(line_t2[0:i])\n",
    "        sentences_t3.append(line_t3[0:i])\n",
    "        sentences_t4.append(line_t4[0:i])\n",
    "        next_chars.append(line[i])\n",
    "\n",
    "        if i==len(line)-1: # special case to deal time of end character\n",
    "            next_chars_t.append(0)\n",
    "            next_chars_t2.append(0)\n",
    "            next_chars_t3.append(0)\n",
    "            next_chars_t4.append(0)\n",
    "        else:\n",
    "            next_chars_t.append(line_t[i])\n",
    "            next_chars_t2.append(line_t2[i])\n",
    "            next_chars_t3.append(line_t3[i])\n",
    "            next_chars_t4.append(line_t4[i])\n",
    "\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239d0a3",
   "metadata": {},
   "source": [
    "# 4. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "928f529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "num features: 11\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "num_features = len(chars)+5\n",
    "print('num features: {}'.format(num_features))\n",
    "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\n",
    "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\n",
    "y_t = np.zeros((len(sentences)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    leftpad = maxlen-len(sentence)\n",
    "    next_t = next_chars_t[i]\n",
    "    sentence_t = sentences_t[i]\n",
    "    sentence_t2 = sentences_t2[i]\n",
    "    sentence_t3 = sentences_t3[i]\n",
    "    sentence_t4 = sentences_t4[i]\n",
    "    for t, char in enumerate(sentence):\n",
    "        multiset_abstraction = Counter(sentence[:t+1])\n",
    "        for c in chars:\n",
    "            if c==char: #this will encode present events to the right places\n",
    "                X[i, t+leftpad, char_indices[c]] = 1\n",
    "        X[i, t+leftpad, len(chars)] = t+1\n",
    "        X[i, t+leftpad, len(chars)+1] = sentence_t[t]/divisor\n",
    "        X[i, t+leftpad, len(chars)+2] = sentence_t2[t]/divisor2\n",
    "        X[i, t+leftpad, len(chars)+3] = sentence_t3[t]/86400\n",
    "        X[i, t+leftpad, len(chars)+4] = sentence_t4[t]/7\n",
    "    for c in target_chars:\n",
    "        if c==next_chars[i]:\n",
    "            y_a[i, target_char_indices[c]] = 1-softness\n",
    "        else:\n",
    "            y_a[i, target_char_indices[c]] = softness/(len(target_chars)-1)\n",
    "    y_t[i] = next_t/divisor\n",
    "    np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd1c55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the training set to 2D\n",
    "X = X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa6699",
   "metadata": {},
   "source": [
    "# 5. Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4e5fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5e3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e49e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2a78367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   8.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   8.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   8.9s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   8.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   8.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  18.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  13.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  13.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  19.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  13.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  16.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  15.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  19.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  16.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  17.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  17.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  18.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  19.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  15.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  15.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  16.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  16.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  18.8s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=5, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   8.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   8.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   9.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   8.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  25.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  25.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  23.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  24.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  27.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  10.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   9.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   9.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  10.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  13.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  13.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   9.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  11.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "                   param_distributions={'max_depth': [None, 5, 10],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 300]},\n",
       "                   scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform grid search to find the best hyperparameters\n",
    "random_search = RandomizedSearchCV(rf_model, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "random_search.fit(X, y_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf41adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None} RandomForestClassifier(min_samples_split=5, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters and model\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search .best_estimator_\n",
    "print(best_params,best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4db82bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(min_samples_split=5, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X, y_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f30c5975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_files/models/bpi_12_w_lowV_RF_best_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model with the best hyperparameters\n",
    "joblib.dump(best_model, f'output_files/models/{eventlog_name}_RF_best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b203eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
